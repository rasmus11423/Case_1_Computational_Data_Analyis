{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN, Decision Tree, Random Forest, SVM, Neural Network + Bagging, Boosting, CV, Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings # to silence convergence warnings\n",
    "# Cross validation, bagging, boosting and oob for optimization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import (RandomForestRegressor, AdaBoostRegressor, \n",
    "                             BaggingRegressor, GradientBoostingRegressor)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor as WeightedKNNRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions from the exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerData(data):\n",
    "    \n",
    "    mu = np.mean(data,axis=0)\n",
    "    data = data - mu\n",
    "    \n",
    "    return data, mu\n",
    "\n",
    "def normalize(X):\n",
    "    '''\n",
    "    Function for normalizing the columns (variables) of a data matrix to unit length.\n",
    "    Returns the normalized data and the L2 norm of the variables \n",
    "    \n",
    "    Input  (X) --------> The data matrix to be normalized \n",
    "    Output (X_pre)-----> The normalized data matrix \n",
    "    Output (d) --------> Array with the L2 norms of the variables \n",
    "    '''\n",
    "    d = np.linalg.norm(X,axis=0,ord=2)  # d is the euclidian lenghts of the variables \n",
    "    d[d==0]=1                           # Avoid dividing by zero if column L2 norm is zero \n",
    "    X_pre = X / d                       # Normalize the data with the euclidian lengths\n",
    "    return X_pre,d                      # Return normalized data and the euclidian lengths\n",
    "\n",
    "def weighted_knn(K, X, n):\n",
    "    yhat = np.zeros(n)\n",
    "    distances = np.zeros(n)\n",
    "    # For each obs, compare distance to all other points in X\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distances[j] = distance.euclidean(X[i,:], X[j, :])\n",
    "\n",
    "        # Sort all the distances\n",
    "        idx = np.argsort(distances)[1:(K + 1)] # Skip first, as distance to \"itself\" does not make sense\n",
    "        Wt = sum(distances[idx]) # Weight of k nearest neighbors\n",
    "        W = distances[idx] / Wt # Weighing average\n",
    "\n",
    "\n",
    "        yhat[i] = np.matmul(W.T, y[idx]) # Final value is weighted combination of neighbours\n",
    "    \n",
    "    return yhat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class WeightedKNNRegressor(BaseEstimator, RegressorMixin):\n",
    "\tdef __init__(self, K):\n",
    "\t\tself.K = K\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\tself.X = X\n",
    "\t\tself.y = y\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\tn = X.shape[0]\n",
    "\t\tyhat = np.zeros(n)\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tdistances = np.array([distance.euclidean(X[i], x) for x in self.X])\n",
    "\t\t\tidx = np.argsort(distances)[:self.K]\n",
    "\t\t\tWt = sum(distances[idx])\n",
    "\t\t\tW = distances[idx] / Wt\n",
    "\t\t\tyhat[i] = np.dot(W, self.y[idx])\n",
    "\t\treturn yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/case1Data.csv')\n",
    "y = np.array(data['y'])\n",
    "X = np.array(data.drop('y', axis=1))\n",
    "data = pd.read_csv('./Data/case1Data_Xnew.csv')\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].fillna(data[col].mean())\n",
    "X_new = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_data(X):\n",
    "    # Round the last 5 columns to integers\n",
    "    X[:, -5:] = np.round(X[:, -5:])\n",
    "\n",
    "    # Convert to integers, then to strings to force categorical treatment\n",
    "    cat_data = X[:, -5:].astype(int).astype(str)\n",
    "    cat_df = pd.DataFrame(cat_data)\n",
    "    cat = pd.get_dummies(cat_df).values\n",
    "\n",
    "    # Separate numerical columns\n",
    "    num = X[:, :-5]\n",
    "    # Standardize numerical columns\n",
    "    num, mu = centerData(num)\n",
    "    num, d = normalize(num)   \n",
    "\n",
    "    # Concatenate numerical and one-hot encoded categorical columns\n",
    "    X = np.concatenate((num, cat), axis=1)\n",
    "\n",
    "    return X, mu, d\n",
    "\n",
    "def transform_data(X, mu, d):\n",
    "    # Round the last 5 columns to integers\n",
    "    X[:, -5:] = np.round(X[:, -5:])\n",
    "\n",
    "    # Convert to integers, then to strings to force categorical treatment\n",
    "    cat_data = X[:, -5:].astype(int).astype(str)\n",
    "    cat_df = pd.DataFrame(cat_data)\n",
    "    cat = pd.get_dummies(cat_df).values\n",
    "\n",
    "    # Separate numerical columns\n",
    "    num = X[:, :-5]\n",
    "    # Standardize numerical columns using the mean and std from the training set\n",
    "    num = (num - mu) / d\n",
    "\n",
    "    # Concatenate numerical and one-hot encoded categorical columns\n",
    "    X = np.concatenate((num, cat), axis=1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=0.9)\n",
    "# pca_plot = PCA(n_components=2)\n",
    "# X_pca = pca.fit_transform(X)\n",
    "# X_pca_plot = pca_plot.fit_transform(X)\n",
    "\n",
    "# X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y, test_size=0.2, random_state=1312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot pca\n",
    "\n",
    "# plt.scatter(X_pca_plot[:,0], X_pca_plot[:,1], c=y, cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model defitinions\n",
    "rf = RandomForestRegressor(random_state=1312)\n",
    "dt = DecisionTreeRegressor(random_state=1312)\n",
    "svr = SVR()\n",
    "ada = AdaBoostRegressor(random_state=1312)\n",
    "bag = BaggingRegressor(random_state=1312)\n",
    "dt = DecisionTreeRegressor(random_state=1312)\n",
    "knn = WeightedKNNRegressor(5)\n",
    "ols = LinearRegression() \n",
    "rr = Ridge() \n",
    "lasso = Lasso()\n",
    "elnet = ElasticNet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERMUTATION 1/5 ===\n",
      "\n",
      "--- VANILLA MODELS ---\n",
      "\n",
      "Evaluating RandomForest...\n",
      "RandomForest - Best params: {'n_estimators': 200, 'max_depth': None}\n",
      "RandomForest - Best CV RMSE: 50.6560\n",
      "\n",
      "Evaluating DecisionTree...\n",
      "DecisionTree - Best params: {'max_depth': None}\n",
      "DecisionTree - Best CV RMSE: 64.4661\n",
      "\n",
      "Evaluating SVR...\n",
      "SVR - Best params: {'C': 10, 'gamma': 'auto'}\n",
      "SVR - Best CV RMSE: 72.4940\n",
      "\n",
      "Evaluating KNN...\n",
      "KNN - Best params: {'n_neighbors': 20}\n",
      "KNN - Best CV RMSE: 73.8020\n",
      "\n",
      "Evaluating AdaBoost...\n",
      "AdaBoost - Best params: {'n_estimators': 200}\n",
      "AdaBoost - Best CV RMSE: 50.2190\n",
      "\n",
      "Evaluating Bagging...\n",
      "Bagging - Best params: {'n_estimators': 500}\n",
      "Bagging - Best CV RMSE: 50.5927\n",
      "\n",
      "Evaluating OLS...\n",
      "OLS - No params to tune - CV RMSE: 57.0995\n",
      "\n",
      "Evaluating Ridge...\n",
      "Ridge - Best params: {'alpha': np.float64(0.042169650342858224)}\n",
      "Ridge - Best CV RMSE: 53.4168\n",
      "\n",
      "Evaluating Lasso...\n",
      "Lasso - Best params: {'alpha': np.float64(0.27384196342643613)}\n",
      "Lasso - Best CV RMSE: 41.3885\n",
      "\n",
      "Evaluating ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': np.float64(0.006493816315762113), 'l1_ratio': 0.9}\n",
      "ElasticNet - Best CV RMSE: 52.6008\n",
      "\n",
      "=== PERMUTATION 2/5 ===\n",
      "\n",
      "--- VANILLA MODELS ---\n",
      "\n",
      "Evaluating RandomForest...\n",
      "RandomForest - Best params: {'n_estimators': 200, 'max_depth': 10}\n",
      "RandomForest - Best CV RMSE: 50.2549\n",
      "\n",
      "Evaluating DecisionTree...\n",
      "DecisionTree - Best params: {'max_depth': 10}\n",
      "DecisionTree - Best CV RMSE: 68.1504\n",
      "\n",
      "Evaluating SVR...\n",
      "SVR - Best params: {'C': 10, 'gamma': 'scale'}\n",
      "SVR - Best CV RMSE: 72.3819\n",
      "\n",
      "Evaluating KNN...\n",
      "KNN - Best params: {'n_neighbors': 20}\n",
      "KNN - Best CV RMSE: 70.1116\n",
      "\n",
      "Evaluating AdaBoost...\n",
      "AdaBoost - Best params: {'n_estimators': 50}\n",
      "AdaBoost - Best CV RMSE: 50.2076\n",
      "\n",
      "Evaluating Bagging...\n",
      "Bagging - Best params: {'n_estimators': 200}\n",
      "Bagging - Best CV RMSE: 50.3376\n",
      "\n",
      "Evaluating OLS...\n",
      "OLS - No params to tune - CV RMSE: 50.8975\n",
      "\n",
      "Evaluating Ridge...\n",
      "Ridge - Best params: {'alpha': np.float64(0.042169650342858224)}\n",
      "Ridge - Best CV RMSE: 47.0107\n",
      "\n",
      "Evaluating Lasso...\n",
      "Lasso - Best params: {'alpha': np.float64(0.27384196342643613)}\n",
      "Lasso - Best CV RMSE: 43.6360\n",
      "\n",
      "Evaluating ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': np.float64(0.006493816315762113), 'l1_ratio': 0.9}\n",
      "ElasticNet - Best CV RMSE: 45.9993\n",
      "\n",
      "=== PERMUTATION 3/5 ===\n",
      "\n",
      "--- VANILLA MODELS ---\n",
      "\n",
      "Evaluating RandomForest...\n",
      "RandomForest - Best params: {'n_estimators': 200, 'max_depth': 10}\n",
      "RandomForest - Best CV RMSE: 50.9328\n",
      "\n",
      "Evaluating DecisionTree...\n",
      "DecisionTree - Best params: {'max_depth': None}\n",
      "DecisionTree - Best CV RMSE: 66.7849\n",
      "\n",
      "Evaluating SVR...\n",
      "SVR - Best params: {'C': 10, 'gamma': 'auto'}\n",
      "SVR - Best CV RMSE: 72.2493\n",
      "\n",
      "Evaluating KNN...\n",
      "KNN - Best params: {'n_neighbors': 20}\n",
      "KNN - Best CV RMSE: 73.6476\n",
      "\n",
      "Evaluating AdaBoost...\n",
      "AdaBoost - Best params: {'n_estimators': 500}\n",
      "AdaBoost - Best CV RMSE: 52.9640\n",
      "\n",
      "Evaluating Bagging...\n",
      "Bagging - Best params: {'n_estimators': 200}\n",
      "Bagging - Best CV RMSE: 51.0163\n",
      "\n",
      "Evaluating OLS...\n",
      "OLS - No params to tune - CV RMSE: 54.7579\n",
      "\n",
      "Evaluating Ridge...\n",
      "Ridge - Best params: {'alpha': np.float64(0.042169650342858224)}\n",
      "Ridge - Best CV RMSE: 52.8571\n",
      "\n",
      "Evaluating Lasso...\n",
      "Lasso - Best params: {'alpha': np.float64(0.27384196342643613)}\n",
      "Lasso - Best CV RMSE: 39.9710\n",
      "\n",
      "Evaluating ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': np.float64(0.006493816315762113), 'l1_ratio': 0.9}\n",
      "ElasticNet - Best CV RMSE: 52.1520\n",
      "\n",
      "=== PERMUTATION 4/5 ===\n",
      "\n",
      "--- VANILLA MODELS ---\n",
      "\n",
      "Evaluating RandomForest...\n",
      "RandomForest - Best params: {'n_estimators': 50, 'max_depth': None}\n",
      "RandomForest - Best CV RMSE: 47.4916\n",
      "\n",
      "Evaluating DecisionTree...\n",
      "DecisionTree - Best params: {'max_depth': 10}\n",
      "DecisionTree - Best CV RMSE: 67.8238\n",
      "\n",
      "Evaluating SVR...\n",
      "SVR - Best params: {'C': 10, 'gamma': 'scale'}\n",
      "SVR - Best CV RMSE: 70.4832\n",
      "\n",
      "Evaluating KNN...\n",
      "KNN - Best params: {'n_neighbors': 20}\n",
      "KNN - Best CV RMSE: 70.2113\n",
      "\n",
      "Evaluating AdaBoost...\n",
      "AdaBoost - Best params: {'n_estimators': 500}\n",
      "AdaBoost - Best CV RMSE: 46.9747\n",
      "\n",
      "Evaluating Bagging...\n",
      "Bagging - Best params: {'n_estimators': 500}\n",
      "Bagging - Best CV RMSE: 47.5714\n",
      "\n",
      "Evaluating OLS...\n",
      "OLS - No params to tune - CV RMSE: 50.8695\n",
      "\n",
      "Evaluating Ridge...\n",
      "Ridge - Best params: {'alpha': np.float64(0.042169650342858224)}\n",
      "Ridge - Best CV RMSE: 48.8435\n",
      "\n",
      "Evaluating Lasso...\n",
      "Lasso - Best params: {'alpha': np.float64(0.27384196342643613)}\n",
      "Lasso - Best CV RMSE: 41.1270\n",
      "\n",
      "Evaluating ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': np.float64(0.006493816315762113), 'l1_ratio': 0.9}\n",
      "ElasticNet - Best CV RMSE: 48.1772\n",
      "\n",
      "=== PERMUTATION 5/5 ===\n",
      "\n",
      "--- VANILLA MODELS ---\n",
      "\n",
      "Evaluating RandomForest...\n",
      "RandomForest - Best params: {'n_estimators': 200, 'max_depth': None}\n",
      "RandomForest - Best CV RMSE: 49.8812\n",
      "\n",
      "Evaluating DecisionTree...\n",
      "DecisionTree - Best params: {'max_depth': 10}\n",
      "DecisionTree - Best CV RMSE: 71.7391\n",
      "\n",
      "Evaluating SVR...\n",
      "SVR - Best params: {'C': 1, 'gamma': 'auto'}\n",
      "SVR - Best CV RMSE: 73.5284\n",
      "\n",
      "Evaluating KNN...\n",
      "KNN - Best params: {'n_neighbors': 20}\n",
      "KNN - Best CV RMSE: 75.8441\n",
      "\n",
      "Evaluating AdaBoost...\n",
      "AdaBoost - Best params: {'n_estimators': 500}\n",
      "AdaBoost - Best CV RMSE: 50.1703\n",
      "\n",
      "Evaluating Bagging...\n",
      "Bagging - Best params: {'n_estimators': 200}\n",
      "Bagging - Best CV RMSE: 49.5027\n",
      "\n",
      "Evaluating OLS...\n",
      "OLS - No params to tune - CV RMSE: 46.8280\n",
      "\n",
      "Evaluating Ridge...\n",
      "Ridge - Best params: {'alpha': np.float64(0.042169650342858224)}\n",
      "Ridge - Best CV RMSE: 44.2165\n",
      "\n",
      "Evaluating Lasso...\n",
      "Lasso - Best params: {'alpha': np.float64(0.27384196342643613)}\n",
      "Lasso - Best CV RMSE: 37.0798\n",
      "\n",
      "Evaluating ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': np.float64(0.006493816315762113), 'l1_ratio': 0.9}\n",
      "ElasticNet - Best CV RMSE: 43.4460\n",
      "\n",
      "=== BOOSTED MODELS ===\n",
      "\n",
      "Evaluating AdaBoost_DecisionTree...\n",
      "AdaBoost_DecisionTree - Best CV RMSE: 49.3377\n",
      "\n",
      "Evaluating GradientBoosting...\n",
      "GradientBoosting - Best CV RMSE: 45.1712\n",
      "\n",
      "Evaluating AdaBoost_OLS...\n",
      "AdaBoost_OLS - Best CV RMSE: 53.9329\n",
      "\n",
      "Evaluating AdaBoost_Ridge...\n",
      "AdaBoost_Ridge - Best CV RMSE: 53.9340\n",
      "\n",
      "Evaluating AdaBoost_Lasso...\n",
      "AdaBoost_Lasso - Best CV RMSE: 38.9194\n",
      "\n",
      "=== BAGGED MODELS ===\n",
      "\n",
      "Evaluating Bagging_RandomForest...\n",
      "Bagging_RandomForest - Best CV RMSE: 51.4180\n",
      "\n",
      "Evaluating Bagging_DecisionTree...\n",
      "Bagging_DecisionTree - Best CV RMSE: 51.9493\n",
      "\n",
      "Evaluating Bagging_KNN...\n",
      "Bagging_KNN - Best CV RMSE: 70.0754\n",
      "\n",
      "Evaluating Bagging_OLS...\n",
      "Bagging_OLS - Best CV RMSE: 53.3748\n",
      "\n",
      "Evaluating Bagging_Ridge...\n",
      "Bagging_Ridge - Best CV RMSE: 54.2198\n",
      "\n",
      "Evaluating Bagging_Lasso...\n",
      "Bagging_Lasso - Best CV RMSE: 39.7918\n",
      "\n",
      "=== FINAL MODEL EVALUATION ON TEST SET ===\n",
      "RandomForest - Test RMSE: 41.7155\n",
      "DecisionTree - Test RMSE: 51.5125\n",
      "SVR - Test RMSE: 72.3045\n",
      "KNN - Test RMSE: 75.3673\n",
      "AdaBoost - Test RMSE: 40.1486\n",
      "Bagging - Test RMSE: 42.6492\n",
      "OLS - Test RMSE: 56.8927\n",
      "Ridge - Test RMSE: 54.4014\n",
      "Lasso - Test RMSE: 31.3573\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'alpha' for estimator BaggingRegressor(estimator=Lasso(alpha=np.float64(0.27384196342643613),\n                                 random_state=1312),\n                 random_state=1312). Valid parameters are: ['bootstrap', 'bootstrap_features', 'estimator', 'max_features', 'max_samples', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 446\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf\u001b[38;5;241m.\u001b[39msplit(X_permuted)):\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# [Keep your existing fold processing code]\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \n\u001b[0;32m    444\u001b[0m     \u001b[38;5;66;03m# Create model with current alpha\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(base_model)\n\u001b[1;32m--> 446\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparam_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[0;32m    449\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_fold_train_scaled, y_fold_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:279\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_params:\n\u001b[0;32m    278\u001b[0m     local_valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names()\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid parameters are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_valid_params\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delim:\n\u001b[0;32m    285\u001b[0m     nested_params[key][sub_key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'alpha' for estimator BaggingRegressor(estimator=Lasso(alpha=np.float64(0.27384196342643613),\n                                 random_state=1312),\n                 random_state=1312). Valid parameters are: ['bootstrap', 'bootstrap_features', 'estimator', 'max_features', 'max_samples', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start']."
     ]
    }
   ],
   "source": [
    "# Custom cross-validation, bagging, boosting and oob for optimization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import (RandomForestRegressor, AdaBoostRegressor, \n",
    "                             BaggingRegressor, GradientBoostingRegressor)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor as WeightedKNNRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "X_dev, X_final_test, y_dev, y_final_test = train_test_split(X, y, test_size=0.2, random_state=1312)\n",
    "\n",
    "columns = []\n",
    "\n",
    "# Define the models and their parameter grids\n",
    "models = {\n",
    "    'RandomForest': (RandomForestRegressor(random_state=1312), \n",
    "                   {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}),\n",
    "    'DecisionTree': (DecisionTreeRegressor(random_state=1312), \n",
    "                   {'max_depth': [None, 10, 20, 30]}),\n",
    "    'SVR': (SVR(), \n",
    "          {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}), \n",
    "    'KNN': (WeightedKNNRegressor(), \n",
    "          {'n_neighbors': [5, 10, 20]}), \n",
    "    'AdaBoost': (AdaBoostRegressor(random_state=1312), \n",
    "               {'n_estimators': [10, 50, 100, 200, 500]}), \n",
    "    'Bagging': (BaggingRegressor(random_state=1312), \n",
    "              {'n_estimators': [10, 50, 100, 200, 500]}),\n",
    "    'OLS': (LinearRegression(), {}),\n",
    "    'Ridge': (Ridge(random_state=1312), \n",
    "            {'alpha': np.logspace(-3, 0.25, num=5)}),\n",
    "    'Lasso': (Lasso(random_state=1312), \n",
    "            {'alpha': np.logspace(-3, 0.25, num=5)}),\n",
    "    'ElasticNet': (ElasticNet(random_state=1312), \n",
    "                 {'alpha': np.logspace(-3, 0.25, num=5), \n",
    "                  'l1_ratio': [0.1, 0.5, 0.7, 0.9]})\n",
    "}\n",
    "\n",
    "# Number of permutations to run\n",
    "n_permutations = 5\n",
    "# Number of CV folds\n",
    "n_splits = 5\n",
    "\n",
    "# Results storage\n",
    "all_results = {\n",
    "    'vanilla': {},\n",
    "    'bagged': {},\n",
    "    'boosted': {}\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_estimators = {}\n",
    "best_params = {}\n",
    "best_cv_scores = {}\n",
    "\n",
    "# OUTER LOOP - handles permutation and data preprocessing\n",
    "for perm_idx in range(n_permutations):\n",
    "    print(f\"\\n=== PERMUTATION {perm_idx+1}/{n_permutations} ===\")\n",
    "    \n",
    "    # 1. Permute the data\n",
    "    random_state = 1312 + perm_idx\n",
    "    indices = np.random.RandomState(random_state).permutation(len(X_dev))\n",
    "    X_permuted, y_permuted = X_dev[indices], y_dev[indices]\n",
    "    \n",
    "    # 2. Set up cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # VANILLA MODELS\n",
    "    print(\"\\n--- VANILLA MODELS ---\")\n",
    "    for model_name, (base_model, param_grid) in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Initialize tracking for this model\n",
    "        if model_name not in best_cv_scores:\n",
    "            best_cv_scores[model_name] = float('inf')\n",
    "            best_params[model_name] = {}\n",
    "            all_results['vanilla'][model_name] = []\n",
    "        \n",
    "        # If no parameters to tune, just do simple CV\n",
    "        if not param_grid:\n",
    "            fold_scores = []\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_permuted)):\n",
    "                # Get fold data\n",
    "                X_fold_train, X_fold_val = X_permuted[train_idx], X_permuted[val_idx]\n",
    "                y_fold_train, y_fold_val = y_permuted[train_idx], y_permuted[val_idx]\n",
    "                \n",
    "                # 3. Fill NA values in this fold\n",
    "                # Convert to DataFrame for easier handling of NaN values\n",
    "                X_fold_train_df = pd.DataFrame(X_fold_train)\n",
    "                X_fold_val_df = pd.DataFrame(X_fold_val)\n",
    "                \n",
    "                # Fill NaN values with mean of training data\n",
    "                for col in X_fold_train_df.columns:\n",
    "                    fill_value = X_fold_train_df[col].mean()\n",
    "                    X_fold_train_df[col] = X_fold_train_df[col].fillna(fill_value)\n",
    "                    X_fold_val_df[col] = X_fold_val_df[col].fillna(fill_value)\n",
    "                \n",
    "                # 4. Standardize the data using training fold stats\n",
    "                X_fold_train_scaled, mu, d = fit_transform_data(X_fold_train_df.values)\n",
    "                X_fold_val_scaled = transform_data(X_fold_val_df.values, mu, d)\n",
    "                \n",
    "                # Ensure both train and val sets have the same columns after one-hot encoding\n",
    "                X_fold_train_scaled = pd.DataFrame(X_fold_train_scaled)\n",
    "                X_fold_val_scaled = pd.DataFrame(X_fold_val_scaled)\n",
    "                X_fold_val_scaled = X_fold_val_scaled.reindex(columns=X_fold_train_scaled.columns, fill_value=0)\n",
    "\n",
    "                # Train and evaluate model\n",
    "                model = copy.deepcopy(base_model)\n",
    "                model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                y_pred = model.predict(X_fold_val_scaled)\n",
    "                mse = mean_squared_error(y_fold_val, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                fold_scores.append(rmse)\n",
    "                \n",
    "            # Average the fold scores\n",
    "            avg_rmse = np.mean(fold_scores)\n",
    "            all_results['vanilla'][model_name].append(avg_rmse)\n",
    "            \n",
    "            print(f\"{model_name} - No params to tune - CV RMSE: {avg_rmse:.4f}\")\n",
    "            \n",
    "            # Check if this is the best score so far\n",
    "            if avg_rmse < best_cv_scores[model_name]:\n",
    "                best_cv_scores[model_name] = avg_rmse\n",
    "                # Train on all dev data for final evaluation\n",
    "                X_dev_df = pd.DataFrame(X_dev)\n",
    "                for col in X_dev_df.columns:\n",
    "                    X_dev_df[col] = X_dev_df[col].fillna(X_dev_df[col].mean())\n",
    "                \n",
    "                X_dev_scaled, mu, d = fit_transform_data(X_dev_df.values)\n",
    "                \n",
    "                scaler = [mu, d]\n",
    "\n",
    "                best_model = copy.deepcopy(base_model)\n",
    "                best_model.fit(X_dev_scaled, y_dev)\n",
    "                best_estimators[model_name] = (best_model, scaler)\n",
    "        \n",
    "        else:\n",
    "            # INNER LOOP - hyperparameter tuning\n",
    "            # Generate all parameter combinations\n",
    "            param_keys = list(param_grid.keys())\n",
    "            param_values = list(param_grid.values())\n",
    "            param_combinations = list(product(*param_values))\n",
    "            \n",
    "            best_avg_rmse = float('inf')\n",
    "            best_param_combo = None\n",
    "            \n",
    "            for param_idx, param_combo in enumerate(param_combinations):\n",
    "                param_dict = {key: value for key, value in zip(param_keys, param_combo)}\n",
    "                \n",
    "                fold_scores = []\n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_permuted)):\n",
    "                    # Get fold data\n",
    "                    X_fold_train, X_fold_val = X_permuted[train_idx], X_permuted[val_idx]\n",
    "                    y_fold_train, y_fold_val = y_permuted[train_idx], y_permuted[val_idx]\n",
    "                    \n",
    "                    # 3. Fill NA values\n",
    "                    X_fold_train_df = pd.DataFrame(X_fold_train)\n",
    "                    X_fold_val_df = pd.DataFrame(X_fold_val)\n",
    "                    \n",
    "                    for col in X_fold_train_df.columns:\n",
    "                        fill_value = X_fold_train_df[col].mean()\n",
    "                        X_fold_train_df[col] = X_fold_train_df[col].fillna(fill_value)\n",
    "                        X_fold_val_df[col] = X_fold_val_df[col].fillna(fill_value)\n",
    "                    \n",
    "                    # 4. Standardize\n",
    "                    X_fold_train_scaled, mu, d = fit_transform_data(X_fold_train_df.values)\n",
    "                    X_fold_val_scaled = transform_data(X_fold_val_df.values, mu, d)\n",
    "\n",
    "                    # Ensure both train and val sets have the same columns after one-hot encoding\n",
    "                    X_fold_train_scaled = pd.DataFrame(X_fold_train_scaled)\n",
    "                    X_fold_val_scaled = pd.DataFrame(X_fold_val_scaled)\n",
    "                    X_fold_val_scaled = X_fold_val_scaled.reindex(columns=X_fold_train_scaled.columns, fill_value=0)\n",
    "\n",
    "                    # Create model with current parameters\n",
    "                    model = copy.deepcopy(base_model)\n",
    "                    model.set_params(**param_dict)\n",
    "                    \n",
    "                    # Train and evaluate\n",
    "                    model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                    y_pred = model.predict(X_fold_val_scaled)\n",
    "                    mse = mean_squared_error(y_fold_val, y_pred)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    fold_scores.append(rmse)\n",
    "                \n",
    "                # Average the fold scores for this parameter combination\n",
    "                avg_rmse = np.mean(fold_scores)\n",
    "                \n",
    "                # Check if this is the best parameter combination\n",
    "                if avg_rmse < best_avg_rmse:\n",
    "                    best_avg_rmse = avg_rmse\n",
    "                    best_param_combo = param_dict\n",
    "            \n",
    "            # Record results for this permutation\n",
    "            all_results['vanilla'][model_name].append(best_avg_rmse)\n",
    "            \n",
    "            print(f\"{model_name} - Best params: {best_param_combo}\")\n",
    "            print(f\"{model_name} - Best CV RMSE: {best_avg_rmse:.4f}\")\n",
    "            \n",
    "            # Check if this is the best score across all permutations\n",
    "            if best_avg_rmse < best_cv_scores[model_name]:\n",
    "                best_cv_scores[model_name] = best_avg_rmse\n",
    "                best_params[model_name] = best_param_combo\n",
    "                \n",
    "                # Train on all dev data with best parameters\n",
    "                X_dev_df = pd.DataFrame(X_dev)\n",
    "                for col in X_dev_df.columns:\n",
    "                    X_dev_df[col] = X_dev_df[col].fillna(X_dev_df[col].mean())\n",
    "                \n",
    "                X_dev_scaled, mu, d = fit_transform_data(X_dev_df.values)\n",
    "                scaler = [mu, d]\n",
    "                \n",
    "                best_model = copy.deepcopy(base_model)\n",
    "                best_model.set_params(**best_param_combo)\n",
    "                best_model.fit(X_dev_scaled, y_dev)\n",
    "                best_estimators[model_name] = (best_model, scaler)\n",
    "            \n",
    "            if model_name == 'Lasso':\n",
    "                # Track alpha values and corresponding RMSE scores\n",
    "                alpha_values = []\n",
    "                rmse_values = []\n",
    "                \n",
    "                # For each parameter combination (in this case, different alpha values)\n",
    "                for param_idx, param_combo in enumerate(param_combinations):\n",
    "                    param_dict = {key: value for key, value in zip(param_keys, param_combo)}\n",
    "                    alpha = param_dict['alpha']\n",
    "                    alpha_values.append(alpha)\n",
    "                    \n",
    "                    fold_scores = []\n",
    "                    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_permuted)):\n",
    "                        # [Keep your existing fold processing code]\n",
    "                        \n",
    "                        # Create model with current alpha\n",
    "                        model = copy.deepcopy(base_model)\n",
    "                        model.set_params(**param_dict)\n",
    "                        \n",
    "                        # Train and evaluate\n",
    "                        model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                        y_pred = model.predict(X_fold_val_scaled)\n",
    "                        mse = mean_squared_error(y_fold_val, y_pred)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        fold_scores.append(rmse)\n",
    "                    \n",
    "                    # Average the fold scores for this alpha\n",
    "                    avg_rmse = np.mean(fold_scores)\n",
    "                    rmse_values.append(avg_rmse)\n",
    "                    \n",
    "                    # [Continue with your existing code]\n",
    "                \n",
    "                # After testing all alphas, plot the results\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.semilogx(alpha_values, rmse_values, marker='o')\n",
    "                plt.xlabel('Alpha (log scale)')\n",
    "                plt.ylabel('RMSE')\n",
    "                plt.title(f'Lasso RMSE vs Alpha (Permutation {perm_idx+1})')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "# After all permutations, evaluate BOOSTED models\n",
    "print(\"\\n=== BOOSTED MODELS ===\")\n",
    "boosted_models = {\n",
    "    'AdaBoost_DecisionTree': (AdaBoostRegressor(\n",
    "        estimator=best_estimators['DecisionTree'][0], n_estimators=100, random_state=1312), {}),\n",
    "    'GradientBoosting': (GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=1312), {}),\n",
    "    'AdaBoost_OLS': (AdaBoostRegressor(\n",
    "        estimator=best_estimators['OLS'][0], n_estimators=100, random_state=1312), {}),\n",
    "    'AdaBoost_Ridge': (AdaBoostRegressor(\n",
    "        estimator=best_estimators['Ridge'][0], n_estimators=100, random_state=1312), {}),\n",
    "    'AdaBoost_Lasso': (AdaBoostRegressor(\n",
    "        estimator=best_estimators['Lasso'][0], n_estimators=100, random_state=1312), {})\n",
    "}\n",
    "\n",
    "# Run similar CV process for boosted models\n",
    "for model_name, (base_model, _) in boosted_models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Initialize tracking\n",
    "    best_cv_scores[model_name] = float('inf')\n",
    "    all_results['boosted'][model_name] = []\n",
    "    \n",
    "    # Run through permutations\n",
    "    for perm_idx in range(n_permutations):\n",
    "        random_state = 1312 + perm_idx\n",
    "        indices = np.random.RandomState(random_state).permutation(len(X_dev))\n",
    "        X_permuted, y_permuted = X_dev[indices], y_dev[indices]\n",
    "        \n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_permuted)):\n",
    "            # Get fold data\n",
    "            X_fold_train, X_fold_val = X_permuted[train_idx], X_permuted[val_idx]\n",
    "            y_fold_train, y_fold_val = y_permuted[train_idx], y_permuted[val_idx]\n",
    "            \n",
    "            # Preprocess\n",
    "            X_fold_train_df = pd.DataFrame(X_fold_train)\n",
    "            X_fold_val_df = pd.DataFrame(X_fold_val)\n",
    "            \n",
    "            for col in X_fold_train_df.columns:\n",
    "                fill_value = X_fold_train_df[col].mean()\n",
    "                X_fold_train_df[col] = X_fold_train_df[col].fillna(fill_value)\n",
    "                X_fold_val_df[col] = X_fold_val_df[col].fillna(fill_value)\n",
    "            \n",
    "            X_fold_train_scaled, mu, d = fit_transform_data(X_fold_train_df.values)\n",
    "            X_fold_val_scaled = transform_data(X_fold_val_df.values, mu, d)\n",
    "\n",
    "            # Make sure both train and val sets have the same columns after one-hot encoding\n",
    "            X_fold_train_scaled = pd.DataFrame(X_fold_train_scaled)\n",
    "            X_fold_val_scaled = pd.DataFrame(X_fold_val_scaled)\n",
    "            X_fold_val_scaled = X_fold_val_scaled.reindex(columns=X_fold_train_scaled.columns, fill_value=0)\n",
    "            \n",
    "            # Train and evaluate\n",
    "            model = copy.deepcopy(base_model)\n",
    "            model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            y_pred = model.predict(X_fold_val_scaled)\n",
    "            mse = mean_squared_error(y_fold_val, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            fold_scores.append(rmse)\n",
    "        \n",
    "        # Average the fold scores\n",
    "        avg_rmse = np.mean(fold_scores)\n",
    "        all_results['boosted'][model_name].append(avg_rmse)\n",
    "        \n",
    "        # Check if this is the best score\n",
    "        if avg_rmse < best_cv_scores[model_name]:\n",
    "            best_cv_scores[model_name] = avg_rmse\n",
    "            \n",
    "            # Train on all dev data\n",
    "            X_dev_df = pd.DataFrame(X_dev)\n",
    "            for col in X_dev_df.columns:\n",
    "                X_dev_df[col] = X_dev_df[col].fillna(X_dev_df[col].mean())\n",
    "            \n",
    "            X_dev_scaled, mu, d = fit_transform_data(X_dev_df.values)\n",
    "            scaler = [mu, d]\n",
    "\n",
    "            best_model = copy.deepcopy(base_model)\n",
    "            best_model.fit(X_dev_scaled, y_dev)\n",
    "            best_estimators[model_name] = (best_model, scaler)\n",
    "    \n",
    "    print(f\"{model_name} - Best CV RMSE: {best_cv_scores[model_name]:.4f}\")\n",
    "\n",
    "# BAGGED models\n",
    "print(\"\\n=== BAGGED MODELS ===\")\n",
    "bagging_models = {\n",
    "    'Bagging_RandomForest': (BaggingRegressor(\n",
    "        estimator=best_estimators['RandomForest'][0], n_estimators=10, random_state=1312), {}),\n",
    "    'Bagging_DecisionTree': (BaggingRegressor(\n",
    "        estimator=best_estimators['DecisionTree'][0], n_estimators=10, random_state=1312), {}),\n",
    "    'Bagging_KNN': (BaggingRegressor(\n",
    "        estimator=best_estimators['KNN'][0], n_estimators=10, random_state=1312), {}),\n",
    "    'Bagging_OLS': (BaggingRegressor(\n",
    "        estimator=best_estimators['OLS'][0], n_estimators=10, random_state=1312), {}),\n",
    "    'Bagging_Ridge': (BaggingRegressor(\n",
    "        estimator=best_estimators['Ridge'][0], n_estimators=10, random_state=1312), {}),\n",
    "    'Bagging_Lasso': (BaggingRegressor(\n",
    "        estimator=best_estimators['Lasso'][0], n_estimators=10, random_state=1312), {})\n",
    "}\n",
    "\n",
    "# Run similar CV process for bagged models\n",
    "for model_name, (base_model, _) in bagging_models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Initialize tracking\n",
    "    best_cv_scores[model_name] = float('inf')\n",
    "    all_results['bagged'][model_name] = []\n",
    "    \n",
    "    # Run through permutations\n",
    "    for perm_idx in range(n_permutations):\n",
    "        random_state = 1312 + perm_idx\n",
    "        indices = np.random.RandomState(random_state).permutation(len(X_dev))\n",
    "        X_permuted, y_permuted = X_dev[indices], y_dev[indices]\n",
    "        \n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_permuted)):\n",
    "            # Get fold data\n",
    "            X_fold_train, X_fold_val = X_permuted[train_idx], X_permuted[val_idx]\n",
    "            y_fold_train, y_fold_val = y_permuted[train_idx], y_permuted[val_idx]\n",
    "            \n",
    "            # Preprocess\n",
    "            X_fold_train_df = pd.DataFrame(X_fold_train)\n",
    "            X_fold_val_df = pd.DataFrame(X_fold_val)\n",
    "            \n",
    "            for col in X_fold_train_df.columns:\n",
    "                fill_value = X_fold_train_df[col].mean()\n",
    "                X_fold_train_df[col] = X_fold_train_df[col].fillna(fill_value)\n",
    "                X_fold_val_df[col] = X_fold_val_df[col].fillna(fill_value)\n",
    "            \n",
    "            X_fold_train_scaled, mu, d = fit_transform_data(X_fold_train_df.values)\n",
    "            X_fold_val_scaled = transform_data(X_fold_val_df.values, mu, d)\n",
    "\n",
    "            # Ensure both train and val sets have the same columns after one-hot encoding\n",
    "            X_fold_train_scaled = pd.DataFrame(X_fold_train_scaled)\n",
    "            X_fold_val_scaled = pd.DataFrame(X_fold_val_scaled)\n",
    "            X_fold_val_scaled = X_fold_val_scaled.reindex(columns=X_fold_train_scaled.columns, fill_value=0)\n",
    "            columns = X_fold_train_scaled.columns.tolist()\n",
    "            \n",
    "            # Train and evaluate\n",
    "            model = copy.deepcopy(base_model)\n",
    "            model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            y_pred = model.predict(X_fold_val_scaled)\n",
    "            mse = mean_squared_error(y_fold_val, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            fold_scores.append(rmse)\n",
    "        \n",
    "        # Average the fold scores\n",
    "        avg_rmse = np.mean(fold_scores)\n",
    "        all_results['bagged'][model_name].append(avg_rmse)\n",
    "        \n",
    "        # Check if this is the best score\n",
    "        if avg_rmse < best_cv_scores[model_name]:\n",
    "            best_cv_scores[model_name] = avg_rmse\n",
    "            \n",
    "            # Train on all dev data\n",
    "            X_dev_df = pd.DataFrame(X_dev)\n",
    "            for col in X_dev_df.columns:\n",
    "                X_dev_df[col] = X_dev_df[col].fillna(X_dev_df[col].mean())\n",
    "            \n",
    "            X_dev_scaled, mu, d = fit_transform_data(X_dev_df.values)\n",
    "            scaler = [mu, d]\n",
    "            \n",
    "            best_model = copy.deepcopy(base_model)\n",
    "            best_model.fit(X_dev_scaled, y_dev)\n",
    "            best_estimators[model_name] = (best_model, scaler)\n",
    "    \n",
    "    print(f\"{model_name} - Best CV RMSE: {best_cv_scores[model_name]:.4f}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "print(\"\\n=== FINAL MODEL EVALUATION ON TEST SET ===\")\n",
    "test_results = {}\n",
    "\n",
    "# Prepare test data\n",
    "X_final_test_df = pd.DataFrame(X_final_test)\n",
    "\n",
    "# Evaluate each model on the test set\n",
    "for model_name, (model, scaler) in best_estimators.items():\n",
    "    # Prepare test data using the same preprocessing as the model was trained with\n",
    "    X_test_processed = X_final_test_df.copy()\n",
    "    \n",
    "    # Fill NAs with training means (stored in scaler)\n",
    "    for col in X_test_processed.columns:\n",
    "        # If test data has NaN values, fill with the mean from training\n",
    "        if X_test_processed[col].isna().any():\n",
    "            # Assuming we can access feature means from the scaler\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(X_test_processed[col].mean())\n",
    "    \n",
    "    # Apply the same scaling as during training\n",
    "    X_test_scaled = transform_data(X_test_processed.values, scaler[0], scaler[1])\n",
    "\n",
    "    # Make sure the test set has the same columns as the training set\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled)\n",
    "    X_test_scaled = X_test_scaled.reindex(columns=columns, fill_value=0)\n",
    "    \n",
    "    # Make predictions and calculate RMSE\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_final_test, y_pred))\n",
    "    test_results[model_name] = test_rmse\n",
    "    \n",
    "    print(f\"{model_name} - Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Find the best model based on test performance\n",
    "best_test_model = min(test_results, key=test_results.get)\n",
    "best_test_rmse = test_results[best_test_model]\n",
    "\n",
    "print(\"\\n=== BEST MODEL SELECTION ===\")\n",
    "print(f\"Best Model on Test Set: {best_test_model}\")\n",
    "print(f\"Best Test RMSE: {best_test_rmse:.4f}\")\n",
    "\n",
    "# If the best model has hyperparameters, show them\n",
    "if best_test_model in best_params and best_params[best_test_model]:\n",
    "    print(f\"Best Parameters: {best_params[best_test_model]}\")\n",
    "\n",
    "# Print summary of all models sorted by test RMSE performance\n",
    "print(\"\\n=== MODEL PERFORMANCE SUMMARY (sorted by Test RMSE) ===\")\n",
    "sorted_models = sorted(test_results.items(), key=lambda x: x[1])\n",
    "for model_name, rmse in sorted_models:\n",
    "    # Identify which category this model belongs to\n",
    "    category = None\n",
    "    if model_name in models:\n",
    "        category = \"vanilla\"\n",
    "    elif model_name in boosted_models:\n",
    "        category = \"boosted\"\n",
    "    elif model_name in bagging_models:\n",
    "        category = \"bagged\"\n",
    "    \n",
    "    print(f\"{model_name} ({category}) - Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "model_categories = {\n",
    "    'vanilla': [m for m in test_results if m in models],\n",
    "    'boosted': [m for m in test_results if m in boosted_models],\n",
    "    'bagged': [m for m in test_results if m in bagging_models]\n",
    "}\n",
    "\n",
    "colors = {'vanilla': 'blue', 'boosted': 'green', 'bagged': 'orange'}\n",
    "for cat, models_list in model_categories.items():\n",
    "    for model in models_list:\n",
    "        plt.bar(model, test_results[model], color=colors[cat], alpha=0.7)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "# # Perform grid search for each model\n",
    "#     best_estimators_pca = {}\n",
    "#     for name, (model, params) in models.items():\n",
    "#         grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error')\n",
    "#         grid_search.fit(X_pca_train, y_pca_train)\n",
    "#         best_estimators_pca[name] = grid_search.best_estimator_\n",
    "#         print(f\"Best parameters for {name} with PCA: {grid_search.best_params_}\")\n",
    "\n",
    "#     # Use boosting to improve the results\n",
    "#     boosted_models_pca = {\n",
    "#         'AdaBoost': AdaBoostRegressor(estimator=best_estimators_pca['DecisionTree'], n_estimators=100, random_state=1312),\n",
    "#         'GradientBoosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=1312),\n",
    "#         'ols': AdaBoostRegressor(estimator=best_estimators_pca['OLS'], n_estimators=100, random_state=1312),\n",
    "#         'Ridge': AdaBoostRegressor(estimator=best_estimators_pca['Ridge'], n_estimators=100, random_state=1312),\n",
    "#         'Lasso': AdaBoostRegressor(estimator=best_estimators_pca['Lasso'], n_estimators=100, random_state=1312)\n",
    "#     }\n",
    "\n",
    "#     # Evaluate the boosted models using cross-validation\n",
    "#     for name, model in boosted_models_pca.items():\n",
    "#         scores_pca = cross_val_score(model, X_pca_train, y_pca_train, cv=5, scoring='neg_mean_squared_error')\n",
    "#         rmse_scores_pca = np.sqrt(-scores_pca)\n",
    "#         print(f\"RMSE for {name} with boosting and PCA: {rmse_scores_pca.mean()}\")\n",
    "\n",
    "#     # Use bagging and cross-validation to estimate the RMSE generalization error\n",
    "#     bagging_models_pca = {\n",
    "#         'RandomForest': BaggingRegressor(estimator=best_estimators_pca['RandomForest'], n_estimators=10, random_state=1312),\n",
    "#         'DecisionTree': BaggingRegressor(estimator=best_estimators_pca['DecisionTree'], n_estimators=10, random_state=1312),\n",
    "#         'KNN': BaggingRegressor(estimator=best_estimators_pca['KNN'], n_estimators=10, random_state=1312),\n",
    "#         'OLS': BaggingRegressor(estimator=best_estimators_pca['OLS'], n_estimators=10, random_state=1312), # Not recommended\n",
    "#         'Ridge': BaggingRegressor(estimator=best_estimators_pca['Ridge'], n_estimators=10, random_state=1312),\n",
    "#         'Lasso': BaggingRegressor(estimator=best_estimators_pca['Lasso'], n_estimators=10, random_state=1312)\n",
    "#     }\n",
    "\n",
    "#     for name, model in bagging_models_pca.items():\n",
    "#         scores_pca = cross_val_score(model, X_pca_train, y_pca_train, cv=5, scoring='neg_mean_squared_error')\n",
    "#         rmse_scores_pca = np.sqrt(-scores_pca)\n",
    "#         print(f\"RMSE for {name} with bagging and PCA: {rmse_scores_pca.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the best models on the test set\n",
    "# for name, model in best_estimators.items():\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#     print(f\"RMSE for {name} on test set: {rmse}\")\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# for name, model in best_estimators_pca.items():\n",
    "#     model.fit(X_pca_train, y_pca_train)\n",
    "#     y_pred_pca = model.predict(X_pca_test)\n",
    "#     rmse_pca = np.sqrt(mean_squared_error(y_pca_test, y_pred_pca))\n",
    "#     print(f\"RMSE for {name} on test set with PCA: {rmse_pca}\")\n",
    "\n",
    "# print(\"\")\n",
    "# print(f\"RMSE for {name} on test set: {np.min(rmse)}\")\n",
    "# print(f\"RMSE for {name} on test set with PCA: {np.min(rmse_pca)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.base import clone\n",
    "\n",
    "def compute_epe_with_small_labeled(model, X_labeled, y_labeled, X_unlabeled, n_bootstraps=100):\n",
    "    \"\"\"\n",
    "    Compute EPE decomposition ( + Bias + Variance) using a small labeled subset and unlabeled data.\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained Lasso model (used for hyperparameters and ).\n",
    "        X_labeled: Labeled subset features (shape: [n_labeled_samples, n_features]).\n",
    "        y_labeled: Labeled subset targets (shape: [n_labeled_samples]).\n",
    "        X_unlabeled: Unlabeled data features (shape: [n_unlabeled_samples, n_features]).\n",
    "        n_bootstraps: Number of bootstrap iterations (default: 100).\n",
    "    \n",
    "    Returns:\n",
    "        epe: Total Expected Prediction Error.\n",
    "        sigma_e_sq: Irreducible error.\n",
    "        bias_sq: Squared bias.\n",
    "        variance: Prediction variance.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute irreducible error () using the labeled subset\n",
    "    y_pred_labeled = model.predict(X_labeled)\n",
    "    sigma_e_sq = np.var(y_labeled - y_pred_labeled, ddof=1)  # Unbiased estimate\n",
    "\n",
    "    # Step 2: Bootstrap training on the labeled subset to estimate bias and variance\n",
    "    n_labeled = X_labeled.shape[0]\n",
    "    n_unlabeled = X_unlabeled.shape[0]\n",
    "    \n",
    "    # Arrays to store predictions for labeled and unlabeled data\n",
    "    preds_labeled = np.zeros((n_bootstraps, n_labeled))\n",
    "    preds_unlabeled = np.zeros((n_bootstraps, n_unlabeled))\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Create bootstrap sample\n",
    "        indices = np.random.choice(n_labeled, n_labeled, replace=True)\n",
    "        X_boot, y_boot = X_labeled[indices], y_labeled[indices]\n",
    "        \n",
    "        # Clone model to retain hyperparameters and train on bootstrap sample\n",
    "        m = clone(model)\n",
    "        m.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Predict on labeled and unlabeled data\n",
    "        preds_labeled[i] = m.predict(X_labeled)\n",
    "        preds_unlabeled[i] = m.predict(X_unlabeled)\n",
    "\n",
    "    # Step 3: Compute bias (using labeled subset)\n",
    "    mean_preds_labeled = np.mean(preds_labeled, axis=0)\n",
    "    bias_sq = np.mean((mean_preds_labeled - y_labeled) ** 2)\n",
    "\n",
    "    # Step 4: Compute variance (using unlabeled data)\n",
    "    variance = np.mean(np.var(preds_unlabeled, axis=0, ddof=1))\n",
    "\n",
    "    # Step 5: Total EPE\n",
    "    epe = sigma_e_sq + bias_sq + variance\n",
    "\n",
    "    return epe, sigma_e_sq, bias_sq, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.3360\n",
      "CV alpha with 1-std-rule: 0.3360\n",
      "Final RMSE on 1std test set: 76.1012\n",
      "Expected Prediction Error 1std (EPE): 1362.0974\n",
      "Irreducible Error 1std (): 473.9171\n",
      "Bias 1std: 606.7378\n",
      "Variance 1std: 281.4425\n",
      "Final RMSE on test set: 78.2153\n",
      "Expected Prediction Error (EPE): 1219.9929\n",
      "Irreducible Error (): 406.4583\n",
      "Bias: 531.9440\n",
      "Variance: 281.5906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of alphas for Lasso\n",
    "alphas = np.logspace(-3, 3, 20)  # Example range of alpha values\n",
    "K = 5  # Number of cross-validation folds\n",
    "\n",
    "X_dev_filled = pd.DataFrame(X_dev).fillna(pd.DataFrame(X_dev).mean()).values\n",
    "X_dev_filled, _, _ = fit_transform_data(X_dev_filled)  # Extract only the transformed data\n",
    "X_final_filled = pd.DataFrame(X_final_test).fillna(pd.DataFrame(X_final_test).mean()).values\n",
    "X_final_filled, _, _ = fit_transform_data(X_final_filled)  # Extract only the transformed data\n",
    "X_new, _, _ = fit_transform_data(X_new)  # Extract only the transformed data\n",
    "\n",
    "# Store RMSE for each alpha\n",
    "RMSE = []\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Perform cross-validation for each alpha\n",
    "    for alpha in alphas:\n",
    "        model = Lasso(alpha=alpha, max_iter=10000)\n",
    "        # Fill missing values in X_dev with the mean of each column\n",
    "\n",
    "        scores = cross_val_score(model, X_dev_filled, y_dev, cv=K, scoring='neg_mean_squared_error')\n",
    "        rmse_scores = np.sqrt(-scores)  # Convert negative MSE to RMSE\n",
    "        RMSE.append(rmse_scores)\n",
    "\n",
    "    RMSE = np.array(RMSE)\n",
    "    meanRMSE = np.mean(RMSE, axis=1)  # Mean RMSE for each alpha\n",
    "    jOpt = np.argmin(meanRMSE)  # Index of the optimal alpha (smallest RMSE)\n",
    "\n",
    "# Calculate the standard error for the best alpha\n",
    "seRMSE = np.std(RMSE, axis=1) / np.sqrt(K)\n",
    "\n",
    "# Find the largest alpha within one standard error of the optimal alpha\n",
    "J = np.where(meanRMSE[jOpt] + seRMSE[jOpt] > meanRMSE)[0]\n",
    "Alpha_CV_1StdRule = alphas[J[-1]]\n",
    "\n",
    "print(f\"Optimal alpha: {alphas[jOpt]:.4f}\")\n",
    "print(f\"CV alpha with 1-std-rule: {Alpha_CV_1StdRule:.4f}\")\n",
    "\n",
    "# Train the final model with the selected alpha\n",
    "final_model = Lasso(alpha=Alpha_CV_1StdRule, max_iter=10000)\n",
    "\n",
    "# Fit the final model on the entire dev set\n",
    "final_model.fit(X_dev_filled, y_dev)\n",
    "# Evaluate the final model on the test set\n",
    "# Making sure the columns are the same in both datasets\n",
    "X_final_filled = pd.DataFrame(X_final_filled)\n",
    "X_final_filled = X_final_filled.reindex(columns=columns, fill_value=0)\n",
    "final_rmse2 = np.sqrt(mean_squared_error(y_final_test, final_model.predict(X_final_filled)))\n",
    "print(f\"Final RMSE on 1std test set: {final_rmse2:.4f}\")\n",
    "\n",
    "epe2, std2, bias2, var2 = compute_epe_with_small_labeled(final_model, X_dev_filled, y_dev, X_new)\n",
    "\n",
    "\n",
    "print(f\"Expected Prediction Error 1std (EPE): {epe2:.4f}\")\n",
    "print(f\"Irreducible Error 1std (): {std2:.4f}\")\n",
    "print(f\"Bias 1std: {bias2:.4f}\")\n",
    "print(f\"Variance 1std: {var2:.4f}\")\n",
    "\n",
    "\n",
    "# Predict on X_new using the final model\n",
    "y_new_pred = final_model.predict(X_new)\n",
    "\n",
    "final_model = Lasso(alpha=np.float64(0.27384196342643613), max_iter=1000) # Use the best model from sorted_models\n",
    "final_model.fit(X_dev_filled, y_dev)\n",
    "# Evaluate the final model on the test set\n",
    "final_rmse = np.sqrt(mean_squared_error(y_final_test, final_model.predict(X_final_filled)))\n",
    "print(f\"Final RMSE on test set: {final_rmse:.4f}\")\n",
    "\n",
    "epe, std, bias, var = compute_epe_with_small_labeled(final_model, X_dev_filled, y_dev, X_new)\n",
    "\n",
    "\n",
    "print(f\"Expected Prediction Error (EPE): {epe:.4f}\")\n",
    "print(f\"Irreducible Error (): {std:.4f}\")\n",
    "print(f\"Bias: {bias:.4f}\")\n",
    "print(f\"Variance: {var:.4f}\")\n",
    "\n",
    "\n",
    "# Predict on X_new using the final model\n",
    "y_new_pred = final_model.predict(X_new)\n",
    "# Save the predicted values to a CSV file\n",
    "#np.savetxt('sample_estimate_prediction_s215160.csv', y_new_pred, delimiter=',',fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected RMSE (using EPE): 36.9066\n",
      "Expected RMSE: 34.9284\n"
     ]
    }
   ],
   "source": [
    "# Expected RMSE\n",
    "rmse_hat = np.sqrt(epe)\n",
    "\n",
    "rmse_hat2 = np.sqrt(epe2)\n",
    "print(f\"Expected RMSE (using EPE): {rmse_hat2:.4f}\")\n",
    "\n",
    "print(f\"Expected RMSE: {rmse_hat:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
